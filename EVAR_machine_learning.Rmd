---
title: "EVAR_machine_learning"
author: "Joshua Geiger and Mohammed Mehdi Shahid"
date: "`r Sys.Date()`"
output:
  word_document: default
  html_document: default
editor_options:
  markdown:
    wrap: 72
---

Update! Dataset is done, and ready to go!

Hey Mehdi,

Thanks for getting this started. The rds dataset that i provided is a
pretty good clean start. The variable that we will be attempting to
predict is sac_regression_12m_cat. This variable states weather The
Shins aortic aneurysms sac regressed states stable or increased in size
at the one-year mark. I had to do a bit of reformatting of all of the
data in order to calculate this appropriately given the variability in
the CT scan dates and sizes. I am confident this variable is accurate at
the moment.

There are variables within the data set that are incomplete and perhaps
chart reviewing to finish collecting some of this data will be
important. Particularly the first couple of patients were recorded
before all of the VQI variables were established and so there is some
missing data there. Additionaly, the last 20 some patients I recently
extracted from the VQI and so variables we manually extracted from the
chart have not be collected yet. Baqir and I can help out with this.
Baqir has also submitted a request for the lab values for these last 20
some patients. In addition we will be obtaining dates of death for the
entire data set. I think doing an additional analysis using similar
methods on survival, either length of survival or survival at 5 years
will also be interesting.


02/26 Meeting (Baqir, Mehdi and Josh)
- Josh to check missing sac_regression data
- Reorder nominal and ordinal variables
- Remove some variables like physician
- Use one-year and 5 year as outcomes too

## SETUP

### Install Packages

```{r setup}
# Installs pacman ("package manager") if needed
if (!require("pacman")) install.packages("pacman")

#Install relevant libraries using pacman (i.e. package manager)

# Use pacman to load add-on packages as desired
pacman::p_load(pacman, 
               # essentials
               tidyverse, rio, caret,finalfit,
               
               #plots
               ggplot2, mice, missRanger,
               
               #machine learning
               pROC, xgboost, ranger, naivebayes, e1071, nnet,
               
               # extras
               forcats, janitor, explore, caTools, Metrics, VIM, car)
```

### Load data

```{r load-data}
#Sourcedir for Mehdi
EVARSourcedir <- "~/Desktop/EVAR/EVAR_cleaned_filtered_12_23_23.rds"

#Sourcedir for Josh
#EVARSourcedir <- "data/EVAR_cleaned_filtered_12_23_23.rds"

EVAR <- readRDS(EVARSourcedir)
```

### Correct Errors

```{r correct-errors}
#Correct errors in the EVAR dataframe
EVAR[ which(EVAR$primprocid == 798089), "height_cm"] <- 188.00
EVAR[ which(EVAR$primprocid == 798089), "bmi"] <- 32.7
```

Let's get a glimpse of the dataframe as it is now
Finalfit is a great package to do this
ff_glimpse() returns a list containing dataframes for continous and categorical vars
It gives mean, min, max, and na_counts and na_percent

```{r EVAR-ff-glimpse}
EVAR_glimpse <- ff_glimpse(EVAR)
```


## CLEANING DATA

Remove cols and rows with too many missing values - remove records with
too many missing values (\>40%) - remove columns with too many missing
values (\>20%)

```{r}

# Remove records with too many missing values
EVAR_preprocess <- EVAR %>%
  select(which(colMeans(is.na(.)) <= 0.6)) #Remove observations with more than 60% missing data

# Remove columns with too many missing values
EVAR_preprocess <- EVAR %>%
  select(which(colMeans(is.na(.)) <= 0.4)) #Remove columns with more than 40% missing data

```

Remove unnecessary or uninformative columns

```{r}
EVAR_preprocess$zip_postal_code <- NULL
EVAR_preprocess$dysrhythmia <- NULL
EVAR_preprocess$primprocid <- NULL

#removing smoking quit date as I don't know how to work with this data type with ML for now....
#I created a years since quite smoking variable and removed the date. 
#EVAR_preprocess$quit_smoking_date <- NULL
```

### Data Types

Change column data types

When converting to factors make sure the ordering makes sense. For
example factor levels for smoking might look something like never, quit,
current. Changing the order to something like quit, current, never,
could very much mess with the analysis if these factors are considered
as ordinal variables, which they often are. can use fct_relevel to
change these

```{r}
# Convert all character columns to factors
EVAR_preprocess <- EVAR_preprocess %>%
  mutate_if(is.character, as.factor) %>%
  # conver to factor if unique values are less or equal to 5
  mutate_if(function(x) is.numeric(x) && n_distinct(x) <= 5, as.factor)

```

#### No and Yes variables

'No' and 'Yes' are mostly nominal, but for consistency, we can reorder
all factor variables with 'No' and 'Yes' to be No for level 1 and Yes
for level 2

```{r}
# Identify factor variables with 'No' and 'Yes' levels
no_yes_factors <- EVAR_preprocess %>%
  select(where(is.factor)) %>%
  keep(~ length(levels(.)) == 2 && all(c("No", "Yes") %in% levels(.)))

# Reorder the levels for 'No' as level 1 and 'Yes' as level 2
EVAR_preprocess <- EVAR_preprocess %>%
  mutate(across(all_of(names(no_yes_factors)), ~fct_relevel(., "No", "Yes")))

```

#### Releveling data

Determine which factors need to be releveled

```{r factor-variables}
# To make sure ordering of each factor variable makes sense, we must investigate each to determine which need to reordered with fct_relevel
# Extract information about factor variables
factorVars_info <- lapply(names(EVAR_preprocess), function(column_name) {
  column <- EVAR_preprocess[[column_name]]
  
  if (is.factor(column)) {
    levels_info <- levels(column)
    return(list(
      variable_name = column_name,
      num_levels = length(levels_info),
      levels = levels_info
    ))
  } else {
    return(NULL)  # Not a factor variable
  }
})

# Remove NULL elements (non-factor variables) from the list
factorVars_info <- factorVars_info[sapply(factorVars_info, function(x) !is.null(x))]

# Print the information
for (info in factorVars_info) {
  cat("Variable Name:", info$variable_name, "\n")
  cat("Number of Levels:", info$num_levels, "\n")
  cat("Levels:", paste(info$levels, collapse = "; "), "\n\n")
}
```

Based on the above output, we choose which variables to reorder and also
correct some errors within the dataset.

#### Convert to numeric

Correcting some errors - aortic_graft_length_1

Convert to num:
aortic_graft_length_1, right_iliac_graft_diameter_1, right_iliac_graft_length_1, left_iliac_graft_length_1, renal_svs, age_svs

```{r correct-errors}
# aortic_graft_length_1 needs some error fixes and then convert to num
# Here we convert one of the errors from 90-30 to 90. Need to check if this is okay....

EVAR_preprocess <- EVAR_preprocess %>%
  mutate(
    # aortic_graft_length_1
    aortic_graft_length_1 = as.character(aortic_graft_length_1), 
    aortic_graft_length_1 = if_else(aortic_graft_length_1 == "90-30", 
                                    "90", aortic_graft_length_1),
    aortic_graft_length_1 = as.numeric(aortic_graft_length_1),
    
    
    # right_iliac_graft_diameter_1
    right_iliac_graft_diameter_1 = as.character(right_iliac_graft_diameter_1), 
    right_iliac_graft_diameter_1 = if_else(right_iliac_graft_diameter_1 == "16-14.5", 
                                           "16", right_iliac_graft_diameter_1),
    right_iliac_graft_diameter_1 = as.numeric(right_iliac_graft_diameter_1),
    
    
    #right_iliac_graft_length_1
    right_iliac_graft_length_1 = as.character(right_iliac_graft_length_1), 
    right_iliac_graft_length_1 = if_else(right_iliac_graft_length_1 == "Other", 
                                           NA, right_iliac_graft_length_1),
    right_iliac_graft_length_1 = as.numeric(right_iliac_graft_length_1),
    
    
    # left_iliac_graft_length_1
    left_iliac_graft_length_1 = as.character(left_iliac_graft_length_1), 
    left_iliac_graft_length_1 = if_else(left_iliac_graft_length_1 == "Other", 
                                           NA, left_iliac_graft_length_1),
    left_iliac_graft_length_1 = as.numeric(left_iliac_graft_length_1),
    
    # renal_svs
    renal_svs = as.numeric(renal_svs),
    
    # age_svs
    age_svs = as.numeric(age_svs),
    )
```

#### Reorder factors
Variables that need to be reordered: 

dysrhy, cvd, prior_chf, copd, diabetes, smoking, 
pre_op_p2y12_antagonist, pre_op_beta_blocker, pre_op_chronic_anticoagulant, ejection_fraction, 
aorta_neck_angle, neck_aaa_angle,
anesthesia, prbc_in_or_or_preop, proximal_aortic_extensions, number_right, number_right
ltf_statin, ltf_beta_blocker, ltf_ace_inhibitor_arb, right_iliac_endpoint, left_iliac_endpoint

```{r reorder-factors}
# dysrhy
EVAR_preprocess$dysrhy <- fct_relevel(EVAR_preprocess$dysrhy, "No", "Yes, atrial", "Other") #N

#cvd
EVAR_preprocess$cvd <- fct_relevel(EVAR_preprocess$cvd,"None","Hx stroke, asymptomatic", "Hx stroke, minor deficit", "Hx stroke, major deficit") #O

#prior_chf
EVAR_preprocess$prior_chf <- fct_relevel(EVAR_preprocess$prior_chf,"None","Asymp, hx CHF", "Mild", "Moderate") #O

#copd
EVAR_preprocess$copd <- fct_relevel(EVAR_preprocess$copd,"No","Not treated","On meds", "On home oxygen") #O

#diabetes
EVAR_preprocess$diabetes <- fct_relevel(EVAR_preprocess$diabetes,"None","Diet","Non-insulin meds", "Insulin") #O

#smoking
EVAR_preprocess$smoking <- fct_relevel(EVAR_preprocess$smoking,"Never", "Prior", "Current") #O


#pre_op_p2y12_antagonist
EVAR_preprocess$pre_op_p2y12_antagonist <- fct_relevel(EVAR_preprocess$pre_op_p2y12_antagonist, "None", "No", "Clopidogrel","Ticagrelor", "Prasugrel" ) # convert to binary (yes/no)

#pre_op_beta_blocker
EVAR_preprocess$pre_op_beta_blocker <- fct_relevel(EVAR_preprocess$pre_op_beta_blocker, "No", "No, for medical reason", "Pre-op 1-30 days", "Chronic > 30 days") # convert to binary (yes/no) 

#pre_op_chronic_anticoagulant
EVAR_preprocess$pre_op_chronic_anticoagulant <- fct_relevel(EVAR_preprocess$pre_op_chronic_anticoagulant, "None", "No, for medical reason", "Other", "Rivaroxaban", "Warfarin", "Dabigatran") # convert to binary (yes/no)

#ejection_fraction
EVAR_preprocess$ejection_fraction <- fct_relevel(EVAR_preprocess$ejection_fraction,"Not done", "Unknown", "<30%", "30-50%", ">50%") # impute unknowns. #O


# Is aorta_neck_angle the same as neck_aaa_angle? 

#aorta_neck_angle
EVAR_preprocess$aorta_neck_angle <- fct_relevel(EVAR_preprocess$aorta_neck_angle, "< 45 Degrees", "45-60 Degrees", "61-75 Degrees", "76-90 Degrees", "> 90 Degrees") #O
#neck_aaa_angle
EVAR_preprocess$neck_aaa_angle <- fct_relevel(EVAR_preprocess$neck_aaa_angle,"< 45 Degrees", "45-60 Degrees", "61-75 Degrees", "76-90 Degrees", "> 90 Degrees") #O


#anesthesia
EVAR_preprocess$anesthesia <- fct_relevel(EVAR_preprocess$anesthesia,"Local", "Regional", "General") #N

EVAR_preprocess$prbc_in_or_or_preop <- fct_relevel(EVAR_preprocess$prbc_in_or_or_preop, "0", "1", "2", "3", "4") # remove

#proximal_aortic_extensions
EVAR_preprocess$proximal_aortic_extensions <- fct_relevel(EVAR_preprocess$proximal_aortic_extensions, "None", "1", "2", "3") #O. Convert none to 0

#number_right
EVAR_preprocess <- EVAR_preprocess %>%
  mutate(number_right = fct_relevel(number_right,"None", "1", "2", "3")) %>%
  mutate(number_right = fct_recode(number_right, "0" = "None")) %>%
  mutate(number_right = as.numeric(as.character(number_right))) #O

# number_left
EVAR_preprocess <- EVAR_preprocess %>%
  mutate(number_left = fct_relevel(number_left,"None", "1", "2", "3")) %>%
  mutate(number_left = fct_recode(number_left, "0" = "None")) %>%
  mutate(number_left = as.numeric(as.character(number_left))) #O

# ltf_statin
EVAR_preprocess <- EVAR_preprocess %>%
  mutate(ltf_statin = as.character(ltf_statin)) %>%
  mutate(ltf_statin = factor(na_if(ltf_statin, ""), levels = c("Yes", "No, for medical reason", "No", "Non-compliant"))) # yes no # N

# ltf_beta_blocker
EVAR_preprocess <- EVAR_preprocess %>%
  mutate(ltf_beta_blocker = as.character(ltf_beta_blocker)) %>%
  mutate(ltf_beta_blocker = factor(na_if(ltf_beta_blocker, ""), levels = c("Yes", "No, for medical reason", "No"))) %>%
  mutate(ltf_beta_blocker = fct_relevel(ltf_beta_blocker, "Yes", "No, for medical reason", "No")) # yes no # N

# ltf_ace_inhibitor_arb
EVAR_preprocess <- EVAR_preprocess %>%
  mutate(ltf_ace_inhibitor_arb = as.character(ltf_ace_inhibitor_arb)) %>%
  mutate(ltf_ace_inhibitor_arb = factor(na_if(ltf_ace_inhibitor_arb, ""), levels = c("Yes", "No, for medical reason", "No"))) %>%
  mutate(ltf_ace_inhibitor_arb = fct_relevel(ltf_ace_inhibitor_arb, "Yes", "No, for medical reason", "No")) # yes no # N

# right_iliac_endpoint
EVAR_preprocess$right_iliac_endpoint <- fct_relevel(EVAR_preprocess$right_iliac_endpoint, "None","Common", "External, Intended", "External, Unintended") #N

# left_iliac_endpoint
EVAR_preprocess$left_iliac_endpoint <- fct_relevel(EVAR_preprocess$left_iliac_endpoint, "None","Common", "External, Intended", "External, Unintended") #N
  
```


Now we generate a glimpse into EVAR_preprocess
```{r EVAR-preprocess-ff-glimpse}
EVAR_preprocess_glimpse <- ff_glimpse(EVAR_preprocess)
```


### Pre-processing report

Generate a report about the data before any pre-processing steps like scaling, and imputation
explore is a nice pkg with function report(). Generates plots and summary stats for all vars in database.

```{r pre-process-report}
EVAR_pre_report_filename <- "Report_EVAR_Preprocess.html"
EVAR_preprocess_report <- report(EVAR_preprocess, 
                                 output_file = EVAR_pre_report_filename,
                                 output_dir = getwd())
```

### Demographics Table

Generate a table1
Finalfit has a function called summary_factorlist() for this
It includes numeric and categorical variables, despite the name...
We want to include the missing data

Stratify against our outcome variable: sac_regression_12m_cat

Currently, there are some warnings: Chi-sq test may be incorrect

```{r table1-demographics}

EVAR_preprocess_predictorsdf <- EVAR_preprocess %>%
  select(-sac_regression_12m_cat)

demographics_table1 <- EVAR_preprocess %>%
  summary_factorlist(dependent = "sac_regression_12m_cat", # outcome variable
  explanatory = names(.)[names(.) != "sac_regression_12m_cat"] , #predictors
  na_include = TRUE,
  na_include_dependent = TRUE,
  total_col = TRUE,
  add_col_totals = TRUE,
  p = TRUE,
  p_cont_para = "aov",
  p_cat = "chisq"
  )


```


### Missing Data Analysis
We want to impute some values. We should determine the nature of our missing data

#### Visualize missing data

```{r visualize-missing}
# Use VIM package
EVAR_preprocess_missing_plot <- aggr(EVAR_preprocess, col=c('navyblue','red'), numbers=TRUE, sortVars=TRUE, labels=names(EVAR_preprocess), cex.axis=.7, gap=3, ylab=c("Histogram of missing data","Pattern"))
```

#### Missingness Pattern
Use md.pattern from mice pkg
We can also use missing_pattern() from finalfit

```{r missingness-pattern}
missingnesspatterndf <- md.pattern(EVAR_preprocess, plot = FALSE)

# num rows is the number of missing patterns
cat("Num of Missing Patterns: ", 
          nrow(missingnesspatterndf), "\n")
```


## PRE-PROCESS

Pipeline: Split -> Impute ->  Feature Selection -> Scale ->  One-hot Encode

### Split Data
Always split first :)

```{r split-data}
set.seed(42)

# split using caTools package
split_sample <- sample.split(EVAR_preprocess$race, 
                             # 70 - 30 split
                             SplitRatio = 0.7)

# Subset train and test data frames
train_df <- subset(EVAR_preprocess, split_sample == TRUE)
test_df <- subset(EVAR_preprocess, split_sample == FALSE)
```


### Imputation

Get a glimpse of training data
```{r train-ff-glimpse}
train_df_glimpse <- ff_glimpse(train_df)
```

We assume data missing at random (MAR)

The imputation strategy is to use missRanger - a fast multiple imputation method using randomForests
Literature shows it is better than standard MICE: 

Suh H, Song J.  A comparison of imputation methods using machine learning models.  CSAM 2023;30:331-341.  https://doi.org/10.29220/CSAM.2023.30.3.331



```{r imputation}
# missRanger

train_imputed <- missRanger(data =  train_df, 
                           # impute all variables using all variables except outcome
                           formula = . ~ . -sac_regression_12m_cat, 
                           pmm.k = 3, num.trees = 100)

test_imputed <- missRanger(data =  test_df, 
                           # impute all variables using all variables except outcome
                           formula = . ~ . -sac_regression_12m_cat, 
                           pmm.k = 3, num.trees = 100)
```

### Non-zero variance Feature Selection

Remove variables with zero variance
```{r zero-variance-vars}

nzv <- nearZeroVar(train_imputed, saveMetrics= T)

# Shortlist of NZV features
nzv_to_remove <- nzv %>% 
    filter(zeroVar==T | nzv==T) %>% 
    rownames_to_column("col_names") 
    
# Let's remove our NZV variables
train_feature_select <- train_imputed %>% 
  select(-pull(nzv_to_remove, col_names))

test_feature_select <- test_imputed %>% 
  select(-pull(nzv_to_remove, col_names))
```


### Correlated Vars
Remove highly correlated variables
```{r}
train_cor <- cor(train_feature_select %>% 
                   select(-sac_regression_12m_cat) %>%
                   summarise(across(where(is.numeric))))
highly_corr_vars <- findCorrelation(train_cor, cutoff = .75)

train_cor_feature_select <- train_feature_select[,-highly_corr_vars]
test_cor_feature_select <- test_feature_select[,-highly_corr_vars]
```


### Ordinal Variables
```{r}
# To make sure ordering of each factor variable makes sense, we must investigate each to determine which need to reordered with fct_relevel
# Extract information about factor variables
ordinalfactorVars_info <- lapply(names(train_cor_feature_select), function(column_name) {
  column <- train_cor_feature_select[[column_name]]
  
  if (is.factor(column)) {
    levels_info <- levels(column)
    return(list(
      variable_name = column_name,
      num_levels = length(levels_info),
      levels = levels_info
    ))
  } else {
    return(NULL)  # Not a factor variable
  }
})

# Remove NULL elements (non-factor variables) from the list
ordinalfactorVars_info <- ordinalfactorVars_info[sapply(train_cor_feature_select, function(x) !is.null(x))]

# Print the information
for (info in ordinalfactorVars_info) {
  cat("Variable Name:", info$variable_name, "\n")
  cat("Number of Levels:", info$num_levels, "\n")
  cat("Levels:", paste(info$levels, collapse = "; "), "\n\n")
}
```
Based on the above, ordinal vars are:
dysrhy, functional_status, cad_symptoms, 
copd, diabetes, smoking, prior_cabg, prior_pci, stress_test, pre_op_p2y12_antagonist, pre_op_beta_blocker, pre_op_chronic_anticoagulant, ejection_fraction, aorta_neck_angle, neck_aaa_angle, discharge_p2y12_antagonist, ltf_asa, ltf_p2y12_antagonist, ltf_statin, ltf_beta_blocker, ltf_ace_inhibitor_arb, ltf_anticoagulant

Here we convert the ordinal variables to be numeric to preserve ordinality. It also helps to reduce dimensionality when we one-hot encode all the other categorical variables

```{r ordinal-vars}
# List of ordinal factor variables
ordinal_factors <- c("dysrhy", "functional_status", "cad_symptoms", 
                     "copd", "diabetes", "smoking", "prior_cabg", "prior_pci", 
                     "stress_test", "pre_op_p2y12_antagonist", "pre_op_beta_blocker", 
                     "pre_op_chronic_anticoagulant", "ejection_fraction", 
                     "aorta_neck_angle", "neck_aaa_angle", 
                     "discharge_p2y12_antagonist", "ltf_asa", "ltf_p2y12_antagonist", 
                     "ltf_statin", "ltf_beta_blocker", "ltf_ace_inhibitor_arb", 
                     "ltf_anticoagulant")

# Convert each ordinal factor variable to numeric preserving ordinality
for (factor_var in ordinal_factors) {
  train_cor_feature_select[[factor_var]] <- as.numeric((train_cor_feature_select[[factor_var]]))
  test_cor_feature_select[[factor_var]] <- as.numeric((test_cor_feature_select[[factor_var]]))

}
```


### Scaling

```{r}

# Apply scaling to numerical columns in train dataframe
train_scaled <- train_cor_feature_select %>%
  mutate_if(is.numeric, ~scales::rescale(., to = c(0, 1)))

# Apply scaling to numerical columns in test dataframe
test_scaled <- test_cor_feature_select %>%
  mutate_if(is.numeric, ~scales::rescale(., to = c(0, 1)))

```


### One-hot encoding
We perform one-hot encoding for every categorical variable
Use dummyVars from caret

We also one-hot encode the outcome variable in order to transform the multi-class classification problem (3 levels: regression, stability and expansion) into three binary classification problems. Each binary outcome variable represents one of the classes versus the rest (often referred to as one-vs-rest or one-vs-all strategy

```{r one-hot-encode}
# fullRank TRUE ensures no linear dependencies induced between the columns
dummyEVAR <- dummyVars(" ~ .", data = train_cor_feature_select, fullRank=T, dummy_na = TRUE)
train_onehot <- data.frame(predict(dummyEVAR, newdata = train_feature_select))
test_onehot <- data.frame(predict(dummyEVAR, newdata = test_feature_select))
```

### NZV for one-hot encode

Perform nzv again as we have one-hot encoded categorical variables. Some have no variance!

Remove variables with zero variance
```{r zero-variance-2-vars}

nzv2 <- nearZeroVar(train_onehot, saveMetrics= T)

# Shortlist of NZV features
nzv_to_remove2 <- nzv2 %>% 
    filter(zeroVar == T | nzv == T) %>% 
    rownames_to_column("col_names") 
    
# Let's remove our NZV variables
train_final <- train_onehot %>% 
  select(-pull(nzv_to_remove2, col_names))

test_final <- test_onehot %>% 
  select(-pull(nzv_to_remove2, col_names))
```



We have finished preprocessing :)
```{r}
EVAR_post_report_filename <- "Report_EVAR_Postprocess.html"
EVAR_postprocess_report <- report(train_scaled, 
                                 output_file = EVAR_post_report_filename,
                                 output_dir = getwd())
```


```{r correlation-matrix}
# Calculate correlation matrix
correlation_matrix <- cor(train_final[, -which(names(train_final) == "sac_regression_12m_cat.Regression")])

# Visualize correlation matrix as a heatmap
heatmap(correlation_matrix)

```


## MODEL TRAINING

### Cross Validation
```{r cross-val}
repeatedcv_trControl <- trainControl(method = "repeatedcv", 
                                     number = 5 , repeats = 10, # 5-fold ; 5 repeats
                                     classProbs = TRUE, # to get class probabilities for ROC curve
                                     summaryFunction = twoClassSummary, # for binary classification
                                     savePredictions = TRUE, # Save predictions for metrics calculation
                          
                                     returnData = FALSE, # Do not return the resampled data
                                     returnResamp = "final", # Only return the final resample result
                                     selectionFunction = "best", # Use the best model based on AUC
                                     allowParallel = TRUE) # Enable parallel processing if available

set.seed(42)
```


### Model Training for Sac Regression

```{r model-train}

# Set model outcome
train_sacreg <- train_final %>%
  mutate(sac_regression_12m_cat.Regression = as.factor(sac_regression_12m_cat.Regression)) %>%
  select(-sac_regression_12m_cat.Expansion)

test_sacreg <- test_final %>%
  mutate(sac_regression_12m_cat.Regression = as.factor(sac_regression_12m_cat.Regression)) %>%
  select(-sac_regression_12m_cat.Expansion)


levels(train_sacreg$sac_regression_12m_cat.Regression) <- c("X0", "X1")
levels(test_sacreg$sac_regression_12m_cat.Regression) <- c("X0", "X1")

```




#### Logistic Regression
```{r logisticreg}

start_time <- Sys.time()

# train model
LR <- train(sac_regression_12m_cat.Regression ~ .,
            data = train_sacreg,
               method = "glm",
               family = "binomial",
               trControl = repeatedcv_trControl,
               metric = "ROC" # Increase max iterations
               )

# Make predictions on the test set
LRpred <- predict(LR,
                  s = LR$bestTune$lambda, # choose the best lambda
                  newdata = test_sacreg %>% select(-sac_regression_12m_cat.Regression), # use the test set
                  type = "prob")

LRROC <- roc(test_sacreg$sac_regression_12m_cat.Regression, LRpred$X1)

total_time <- Sys.time() - start_time
cat("LogisticRegression time", total_time, "\n")

```


#### RF
```{r rf}
# Random Forest Model
start_time <- Sys.time()

# specify tuning grid for optimum hyperparameters
RFgrid <- expand.grid(mtry = 2:4, 
                      splitrule = "gini", 
                      min.node.size = c(10,20))

levels(train_sacreg$sac_regression_12m_cat.Regression) <- c("X0", "X1")

# train RF model
RF <- train(sac_regression_12m_cat.Regression ~ .,
            data = train_sacreg,
            method = "ranger",
            trControl = repeatedcv_trControl,
            tuneGrid = RFgrid,
            metric = "ROC")

# predict on test set
RFpred <- predict(RF, newdata = test_sacreg %>% select(-sac_regression_12m_cat.Regression), # use the test set
                         type = "prob")

RFROC <- roc(test_sacreg$sac_regression_12m_cat.Regression, RFpred$X1)

total_time <- Sys.time() - start_time
cat("RF time", total_time, "\n")
```

#### XGBoost Tree

Takes a while to run. XGBoost can be computationally expensive

```{r xgboost}
# XGBoostTree Model
start_time <- Sys.time()

# specify tuning grid for optimum hyperparameters
XGBgrid <- expand.grid(nrounds = (1:5)*50,
                       max_depth = 2:5,
                       eta = c(0.4,0.1,0.001),
                       min_child_weight = c(1,5,10),
                       subsample = c(0.5,0.7,0.9,1),
                       gamma = c(0,0.1,1,2),
                       colsample_bytree = c(0.5,0.7,0.9,1))
# train RF model
XGB <- train(sac_regression_12m_cat.Regression ~ .,
            data = train_sacreg,
            method = "xgbTree",
            trControl = repeatedcv_trControl,
            tuneGrid = XGBgrid,
            metric = "ROC")

# predict on test set
XGBpred <- predict(XGB, newdata = test_sacreg %>% select(-sac_regression_12m_cat.Regression), # use the test set
                         type = "prob")

XGBROC <- roc(test_sacreg$sac_regression_12m_cat.Regression, XGBpred$X1)

total_time <- Sys.time() - start_time
cat("XGB time", total_time, "\n")
```


#### Naive Bayes
```{r naive-bayes}
# Naive Bayes Model

start_time <- Sys.time()
# specify tuning grid for optimum hyperparameters
NBgrid <- expand.grid(
  laplace = c(0, 0.5, 1.0), 
  usekernel = c(TRUE,FALSE), 
  adjust = c(0, 0.5, 1.0))
# train RF model
NB <- train(sac_regression_12m_cat.Regression ~ .,
            data = train_sacreg,
            method = "naive_bayes",
            trControl = repeatedcv_trControl,
            tuneGrid = NBgrid,
            metric = "ROC")

# predict on test set
NBpred <- predict(NB, newdata = test_sacreg %>% select(-sac_regression_12m_cat.Regression), # use the test set
                         type = "prob")

NBROC <- roc(test_sacreg$sac_regression_12m_cat.Regression, NBpred$X1)

total_time <- Sys.time() - start_time
cat("NB time", total_time, "\n")
```


#### Support Vector Machines
```{r svm}
# SVM model
start_time <- Sys.time()

# specify tuning grid for optimum hyperparameters
SVMgrid <- expand.grid(sigma = c(0.01, 0.05, 0.1, 1),
                          C = c(0.01, 0.05, 0.1, 1))
# train RF model
SVM <- train(sac_regression_12m_cat.Regression ~ .,
            data = train_sacreg,
            method = "svmRadial",
            trControl = repeatedcv_trControl,
            tuneGrid = SVMgrid,
            metric = "ROC")

# predict on test set
SVMpred <- predict(SVM, newdata = test_sacreg %>% select(-sac_regression_12m_cat.Regression), # use the test set
                         type = "prob")

SVMROC <- roc(test_sacreg$sac_regression_12m_cat.Regression, SVMpred$X1)

total_time <- Sys.time() - start_time
cat("SVM time", total_time, "\n")

```


#### NNETgrid
```{r nnetgrid}
# NNETgrid model
# multilayer perceptron artificial neural network
start_time <- Sys.time()

# specify tuning grid for optimum hyperparameters
NNETgrid <- expand.grid(size = seq(from = 1, to = 10, by = 1),
                        decay = seq(from = 0.1, to = 0.5, by = 0.1))
# train RF model
NNET <- train(sac_regression_12m_cat.Regression ~ .,
            data = train_sacreg,
            method = "nnet",
            trControl = repeatedcv_trControl,
            tuneGrid = NNETgrid,
            metric = "ROC")

# predict on test set
NNETpred <- predict(NNET, newdata = test_sacreg %>% select(-sac_regression_12m_cat.Regression), # use the test set
                         type = "prob")

NNETROC <- roc(test_sacreg$sac_regression_12m_cat.Regression, NNETpred$X1)

total_time <- Sys.time() - start_time
cat("NNET time", total_time, "\n")

```


# END
```{r}

```

\`\`\`
